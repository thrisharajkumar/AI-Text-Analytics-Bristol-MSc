{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8682f072",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) for Twitter Data\n",
    "**Dataset:** Broad Twitter Corpus (BTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471a3599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\pc\\anaconda3\\envs\\text_analytics\\lib\\site-packages\\huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets nltk sklearn-crfsuite emoji tqdm\n",
    "# installing the libraries required for the task\n",
    "# datasets to load the Broad Twitter dataset using HuggingFace datasets\n",
    "# nltk for NLP preprocessing steps such as POStags, stopword filtering and tokenization\n",
    "# sklearn-crfsuite for CRF model\n",
    "# emoji for emoji processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a3f9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\pc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\pc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\pc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\pc/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the required libraies for TASK CRF modelling \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn_crfsuite import CRF, metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7896d229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[I, hate, the, words, chunder, ,, vomit, and, ...</td>\n",
       "      <td>[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[♥, ., ., ), ), (, ♫, ., (, ړײ, ), ♫, ., ♥, .,...</td>\n",
       "      <td>[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Alesan, kenapa, mlm, kita, lbh, srg, galau, P...</td>\n",
       "      <td>[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Complete, Tosca, on, the, tube, http://t.co/O...</td>\n",
       "      <td>[6, 6, 6, 6, 6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Think, you, call, that, smash, and, grab, ., ...</td>\n",
       "      <td>[6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [I, hate, the, words, chunder, ,, vomit, and, ...   \n",
       "1  [♥, ., ., ), ), (, ♫, ., (, ړײ, ), ♫, ., ♥, .,...   \n",
       "2  [Alesan, kenapa, mlm, kita, lbh, srg, galau, P...   \n",
       "3  [Complete, Tosca, on, the, tube, http://t.co/O...   \n",
       "4  [Think, you, call, that, smash, and, grab, ., ...   \n",
       "\n",
       "                                                tags  \n",
       "0               [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]  \n",
       "1  [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...  \n",
       "2  [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...  \n",
       "3                                 [6, 6, 6, 6, 6, 6]  \n",
       "4  [6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading thw Broad Twitter dataset\n",
    "# creating dataframes for the train, test and validation data splits\n",
    "dataset = load_dataset(\"tner/btc\")\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "valid_df = pd.DataFrame(dataset['validation'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "# checking the head to check the training data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf3a385",
   "metadata": {},
   "source": [
    "#### POS tagging and Feature Engineering for CRF model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# In each row joiing the tokens to make a sentence and get the Parts-of-Speech tags \n",
    "train_df['pos_tags'] = train_df['tokens'].apply(lambda tokens: [token.pos_ for token in nlp(' '.join(tokens))])\n",
    "valid_df['pos_tags'] = valid_df['tokens'].apply(lambda tokens: [token.pos_ for token in nlp(' '.join(tokens))])\n",
    "test_df['pos_tags'] = test_df['tokens'].apply(lambda tokens: [token.pos_ for token in nlp(' '.join(tokens))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e22d8",
   "metadata": {},
   "source": [
    "#### Token Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "249f9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# As the twitter data contains emojis it iwill create noise intead of removing it we can retain it in the form of words o tokens\n",
    "# the emojis textual meaning using the emoji library\n",
    "def convert_emojis(tokens):\n",
    "    return [emoji.demojize(token).replace(\":\", \"\").replace(\"_\", \" \") if token in emoji.EMOJI_DATA else token for token in tokens]\n",
    "def preprocess_tokens(tokens):\n",
    "    tokens = [token.lower() for token in tokens] #lowercasing and normalization of the text\n",
    "    tokens = convert_emojis(tokens)  # converted the emojis \n",
    "    tokens = [token for token in tokens if token not in stop_words] # removed the stop words like the, and, or etc. \n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # reducing the words to its root fofrm using the lemmatization technique \n",
    "\n",
    "    tokens = [token for token in tokens if re.match(r'[A-Za-z]+', token)] # this pny retains the necessary text data and removes the punctuations and other special tokens \n",
    "    return tokens\n",
    "train_df['processed_tokens'] = train_df['tokens'].apply(preprocess_tokens)\n",
    "valid_df['processed_tokens'] = valid_df['tokens'].apply(preprocess_tokens)\n",
    "test_df['processed_tokens'] = test_df['tokens'].apply(preprocess_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51db409",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e5f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction for the CRF is one of the important NLP steps to acwuire the contextual features\n",
    "def word2features(sent, i):\n",
    "    word = sent[i]\n",
    "    # For each word getting the lexical features \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True # Beginning boundaryof the sentences\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True # End boundary of the sentences\n",
    "\n",
    "    return features\n",
    "\n",
    "# extracting features for all words in a token sequence\n",
    "def extract_features(tokens):\n",
    "    return [word2features(tokens, i) for i in range(len(tokens))]\n",
    "# BIO formatting for NER CRF modellong\n",
    "def get_labels(entities):\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f497225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens feature extracction for all the datsets train, test and Validation\n",
    "X_train = [extract_features(s) for s in train_df['tokens']]\n",
    "y_train = [get_labels(s) for s in train_df['tags']]\n",
    "X_valid = [extract_features(s) for s in valid_df['tokens']]\n",
    "y_valid = [get_labels(s) for s in valid_df['tags']]\n",
    "X_test = [extract_features(s) for s in test_df['tokens']]\n",
    "y_test = [get_labels(s) for s in test_df['tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75d4b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label mapping the dataset features\n",
    "label_list = dataset['train'].features['tags'].feature.names\n",
    "def decode_labels(label_ids):\n",
    "    return [label_list[i] for i in label_ids]\n",
    "# again apply to the datsets \n",
    "y_train = train_df['tags'].apply(decode_labels).tolist()\n",
    "y_valid = valid_df['tags'].apply(decode_labels).tolist()\n",
    "y_test = test_df['tags'].apply(decode_labels).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47428884",
   "metadata": {},
   "source": [
    "## CRF Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "031942a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, max_iterations=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>CRF</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, max_iterations=100)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, max_iterations=100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = CRF(algorithm='lbfgs', max_iterations=100, all_possible_transitions=True)\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568dd6a6",
   "metadata": {},
   "source": [
    "#### Evaluating on Test data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4447bdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.765     0.395     0.521       636\n",
      "       B-PER      0.796     0.816     0.806      2650\n",
      "       B-ORG      0.599     0.237     0.339      1090\n",
      "       I-ORG      0.474     0.301     0.368       246\n",
      "       I-PER      0.608     0.662     0.633       269\n",
      "       I-LOC      0.654     0.337     0.444       208\n",
      "\n",
      "   micro avg      0.743     0.587     0.656      5099\n",
      "   macro avg      0.649     0.458     0.519      5099\n",
      "weighted avg      0.719     0.587     0.626      5099\n",
      "\n",
      "Token-level Accuracy: 0.9343\n"
     ]
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "# Excluding 'O' from the labels so we can get more better results\n",
    "labels = list(crf.classes_)\n",
    "if 'O' in labels:\n",
    "    labels.remove('O')\n",
    "\n",
    "print(\"CRF Classification:\")\n",
    "print(metrics.flat_classification_report(y_test, y_pred, labels=labels, digits=3))\n",
    "\n",
    "correct = sum(p == t for seq_p, seq_t in zip(y_pred, y_test) for p, t in zip(seq_p, seq_t))\n",
    "total = sum(len(seq) for seq in y_test)\n",
    "accuracy = correct / total\n",
    "print(\"Token-level Accuracy:\", round(accuracy, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb2dda06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report CRF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC      0.830     0.386     0.527       844\n",
      "         ORG      0.683     0.258     0.375      1336\n",
      "         PER      0.814     0.811     0.812      2919\n",
      "\n",
      "   micro avg      0.798     0.596     0.682      5099\n",
      "   macro avg      0.775     0.485     0.571      5099\n",
      "weighted avg      0.782     0.596     0.650      5099\n",
      "\n",
      "\n",
      "Token-level Accuracy: 0.9355\n"
     ]
    }
   ],
   "source": [
    "# perfroming the CRF on the test data and classifying on LOC, ORG and PER \n",
    "\n",
    "def convert_to_coarse_tags(tag_sequence):\n",
    "    return [tag.split('-')[-1] if tag != 'O' else tag for tag in tag_sequence]\n",
    "\n",
    "from sklearn_crfsuite import metrics as crf_metrics\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "def decode_labels(tags): return [label_list[t] for t in tags]\n",
    "y_test = [decode_labels(seq) for seq in test_df['tags']]\n",
    "\n",
    "y_test_coarse = [convert_to_coarse_tags(seq) for seq in y_test]\n",
    "y_pred_coarse = [convert_to_coarse_tags(seq) for seq in y_pred]\n",
    "# Excluded the 'O' labels\n",
    "def filter_out_O(y_true, y_pred):\n",
    "    y_true_filtered, y_pred_filtered = [], []\n",
    "    for t_seq, p_seq in zip(y_true, y_pred):\n",
    "        filtered_t = []\n",
    "        filtered_p = []\n",
    "        for t, p in zip(t_seq, p_seq):\n",
    "            if t != 'O':\n",
    "                filtered_t.append(t)\n",
    "                filtered_p.append(p)\n",
    "        if filtered_t:\n",
    "            y_true_filtered.append(filtered_t)\n",
    "            y_pred_filtered.append(filtered_p)\n",
    "    return y_true_filtered, y_pred_filtered\n",
    "\n",
    "y_test_filtered, y_pred_filtered = filter_out_O(y_test_coarse, y_pred_coarse)\n",
    "\n",
    "\n",
    "# only LOC, ORG and PER for the task as required \n",
    "labels = ['LOC', 'ORG', 'PER']  \n",
    "print(\"Classification Report CRF:\")\n",
    "print(crf_metrics.flat_classification_report(\n",
    "    y_test_filtered, y_pred_filtered, labels=labels, digits=3\n",
    "))\n",
    "\n",
    "correct = sum(p == t for seq_p, seq_t in zip(y_pred_coarse, y_test_coarse) for p, t in zip(seq_p, seq_t))\n",
    "total = sum(len(seq) for seq in y_test_coarse)\n",
    "accuracy = correct / total\n",
    "print(f\"\\nToken-level Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54f9c726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Entity Span (BIO):\n",
      "It           -> O\n",
      "'s           -> O\n",
      "like         -> O\n",
      "this         -> O\n",
      "Hunico       -> B-PER\n",
      "/            -> I-PER\n",
      "TedDiBiase   -> O\n",
      "match        -> O\n",
      "did          -> O\n",
      "n't          -> O\n",
      "happen       -> O\n",
      ".            -> O\n",
      "Cole         -> B-PER\n",
      "&            -> O\n",
      "Josh         -> B-PER\n",
      "did          -> O\n",
      "n't          -> O\n",
      "even         -> O\n",
      "call         -> O\n",
      "it           -> O\n",
      ",            -> O\n",
      "they         -> O\n",
      "just         -> O\n",
      "spoke        -> O\n",
      "over         -> O\n",
      "it           -> O\n",
      "with         -> O\n",
      "nonsense     -> O\n",
      "#            -> O\n",
      "wwe          -> B-ORG\n"
     ]
    }
   ],
   "source": [
    "# example of the tokens and their BIO-encoded tags\n",
    "example_index = 200 # any row\n",
    "example_tokens = train_df['tokens'][example_index]\n",
    "example_tags = decode_labels(train_df['tags'][example_index])\n",
    "print(\"Example Entity Span (BIO):\")\n",
    "for token, tag in zip(example_tokens, example_tags):\n",
    "    print(f\"{token:<12} -> {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9ff88fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.829517</td>\n",
       "      <td>0.386256</td>\n",
       "      <td>0.527082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.683168</td>\n",
       "      <td>0.258234</td>\n",
       "      <td>0.374796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.813682</td>\n",
       "      <td>0.810894</td>\n",
       "      <td>0.812286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     precision    recall  f1-score\n",
       "LOC   0.829517  0.386256  0.527082\n",
       "ORG   0.683168  0.258234  0.374796\n",
       "PER   0.813682  0.810894  0.812286"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn.metrics import classification_report as sk_class_report\n",
    "\n",
    "report = flat_classification_report(y_test_filtered, y_pred_filtered, labels=['LOC', 'ORG', 'PER'], digits=3, output_dict=True)\n",
    "crf_results_df = pd.DataFrame(report).T\n",
    "crf_results_df = crf_results_df[['precision', 'recall', 'f1-score']]\n",
    "crf_results_df.drop(index=['micro avg', 'macro avg', 'weighted avg', 'samples avg'], errors='ignore', inplace=True)\n",
    "print(\"CRF Evaluation Table:\")\n",
    "display(crf_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c27375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of CRF Misclassification:\n",
      "Tokens       : ['This', 'morning', 'I', 'met', 'with', 'Senators', 'Inabo', 'and', 'Senior', 'from', 'Palau', 'to', 'discuss', 'my', 'role', 'as', 'Chair', 'of', 'the', 'Public', 'Works', '.', '.', '.', 'http://t.co/McYNwpzHmt']\n",
      "True Labels  : ['PER', 'PER', 'LOC']\n",
      "Pred Labels  : ['ORG', 'O', 'LOC']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample of CRF Misclassification:\")\n",
    "for i, (true_seq, pred_seq, tokens) in enumerate(zip(y_test_filtered, y_pred_filtered, test_df['tokens'])):\n",
    "    if true_seq != pred_seq:\n",
    "        print(f\"Tokens       : {tokens}\")\n",
    "        print(f\"True Labels  : {true_seq}\")\n",
    "        print(f\"Pred Labels  : {pred_seq}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3002b285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Tokens Used for CRF Feature Extraction:\n",
      "\n",
      "Example 1:\n",
      "['hate', 'word', 'chunder', 'vomit', 'puke', 'buuh']\n",
      "--------------------------------------------------\n",
      "Example 2:\n",
      "['heart suit', 'heart suit', 'heart suit', 'happy', 'new', 'year']\n",
      "--------------------------------------------------\n",
      "Example 3:\n",
      "['alesan', 'kenapa', 'mlm', 'kita', 'lbh', 'srg', 'galau', 'poconggg', 'twitfakta', 'otak', 'lebih', 'aktif', 'di', 'malam', 'hari', 'dari', 'pada', 'di', 'pagi', 'hari', 'twitfakta']\n",
      "--------------------------------------------------\n",
      "Example 4:\n",
      "['complete', 'tosca', 'tube', 'http://t.co/o90deslb']\n",
      "--------------------------------------------------\n",
      "Example 5:\n",
      "['think', 'call', 'smash', 'grab', 'gateshead', 'medium', 'man', 'admitted', 'daylight', 'robbery', 'shaw', 'touch', 'goal']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# the processed tokens used in the CRF model\n",
    "\n",
    "def print_processed_tokens(dataset_split, num_examples=5):\n",
    "    print(\"\\nProcessed Tokens Used for CRF Feature Extraction:\\n\")\n",
    "    for i in range(num_examples):\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(dataset_split['processed_tokens'].iloc[i])\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "print_processed_tokens(train_df, num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f6d36ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correctly Predicted Tokens for PER:\n",
      "\n",
      "Token: colgo, True Label: PER, Predicted Label: PER\n",
      "Token: alisoncroggon, True Label: PER, Predicted Label: PER\n",
      "Token: AmandaRishworth, True Label: PER, Predicted Label: PER\n",
      "Token: Robin, True Label: PER, Predicted Label: PER\n",
      "Token: Williams, True Label: PER, Predicted Label: PER\n",
      "\n",
      "Correctly Predicted Tokens for ORG:\n",
      "\n",
      "Token: New, True Label: ORG, Predicted Label: ORG\n",
      "Token: York, True Label: ORG, Predicted Label: ORG\n",
      "Token: Times, True Label: ORG, Predicted Label: ORG\n",
      "Token: ADF, True Label: ORG, Predicted Label: ORG\n",
      "Token: NTThunderFC, True Label: ORG, Predicted Label: ORG\n",
      "\n",
      "Correctly Predicted Tokens for LOC:\n",
      "\n",
      "Token: Palau, True Label: LOC, Predicted Label: LOC\n",
      "Token: Aston, True Label: LOC, Predicted Label: LOC\n",
      "Token: Parlt, True Label: LOC, Predicted Label: LOC\n",
      "Token: Australia, True Label: LOC, Predicted Label: LOC\n",
      "Token: Canberra, True Label: LOC, Predicted Label: LOC\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# finding the correctly predicted tokens for each entity type toaddress the misclassification\n",
    "def print_correct_predictions_by_entity(entity_type, y_true, y_pred, tokens_list, num_examples=5):\n",
    "   \n",
    "    count = 0\n",
    "    special_token_pattern = re.compile(r'^[a-zA-Z]+$') \n",
    "    print(f\"\\nCorrectly Predicted Tokens for {entity_type}:\\n\")\n",
    "    for true_seq, pred_seq, tokens in zip(y_true, y_pred, tokens_list):\n",
    "        for token, true_label, pred_label in zip(tokens, true_seq, pred_seq):\n",
    "            if (true_label == entity_type) and (pred_label == entity_type) and special_token_pattern.match(token):\n",
    "                print(f\"Token: {token}, True Label: {true_label}, Predicted Label: {pred_label}\")\n",
    "                count += 1\n",
    "                if count >= num_examples:\n",
    "                    return\n",
    "\n",
    "print_correct_predictions_by_entity('PER', y_test_coarse, y_pred_coarse, test_df['tokens'], num_examples=5)\n",
    "print_correct_predictions_by_entity('ORG', y_test_coarse, y_pred_coarse, test_df['tokens'], num_examples=5)\n",
    "print_correct_predictions_by_entity('LOC', y_test_coarse, y_pred_coarse, test_df['tokens'], num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "518f7fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity span undersanding about the BIO tagging \n",
    "def extract_entity_spans(tokens, tags):\n",
    "    span = []\n",
    "    current_entity = None\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if span:\n",
    "                print(f\"Entity: {' '.join(span)} | Type: {current_entity}\")\n",
    "                span = []\n",
    "            current_entity = tag[2:]\n",
    "            span = [token]\n",
    "        elif tag.startswith(\"I-\") and current_entity == tag[2:]:\n",
    "            span.append(token)\n",
    "        else:\n",
    "            if span:\n",
    "                print(f\"Entity: {' '.join(span)} | Type: {current_entity}\")\n",
    "                span = []\n",
    "            current_entity = None\n",
    "    if span:\n",
    "        print(f\"Entity: {' '.join(span)} | Type: {current_entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "871e965d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: New York Times | Type: ORG\n",
      "Entity: Barack Obama | Type: PER\n"
     ]
    }
   ],
   "source": [
    "tokens = ['New', 'York', 'Times', 'is', 'great', 'and', 'Barack', 'Obama', 'was', 'there']\n",
    "tags = ['B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O']\n",
    "extract_entity_spans(tokens, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377158c9",
   "metadata": {},
   "source": [
    "# BiLSTM + CRF for Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693e3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\pc\\anaconda3\\envs\\text_analytics\\lib\\site-packages\\huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch torchcrf seqeval \n",
    "# Torch is needed for the BiLSTM model training\n",
    "# torchcrf for the CRF Layer which is on top of the BiLSTM model\n",
    "# seqeval for perfromance metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47cec8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries required for the BiLST+ CRF modelling \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7394d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tner/btc\")\n",
    "\n",
    "# \n",
    "label_list = dataset['train'].features['tags'].feature.names\n",
    "\n",
    "# this function is necesary for later on steps - it converts numerical tag IDs to BIO formatting\n",
    "# we need this for the viewing the seqeval predictions as it is showing error for the numerical IDs \n",
    "# Also helped to see the BIO tagging and help understand the classes or the labelling of the tokens \n",
    "def decode_labels(tags): return [label_list[tag] for tag in tags]\n",
    "\n",
    "# created new dataframs for this model to avoid confusion\n",
    "train_tokens = dataset['train']['tokens']\n",
    "train_labels = [decode_labels(x) for x in dataset['train']['tags']]\n",
    "val_tokens = dataset['validation']['tokens']\n",
    "val_labels = [decode_labels(x) for x in dataset['validation']['tags']]\n",
    "test_tokens = dataset['test']['tokens']\n",
    "test_labels = [decode_labels(x) for x in dataset['test']['tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e435ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#  creating the word vocabulary\n",
    "#  case folding - lowercasing  similar to the CRF model\n",
    "all_words = [w.lower() for sent in train_tokens for w in sent]\n",
    "word_counter = Counter(all_words) # number of words \n",
    "vocab = [\"<PAD>\", \"<UNK>\"] + [w for w, c in word_counter.items() if c > 1]\n",
    "# word-to-index mapping dictionary\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "# Character level vocabulary extrcation\n",
    "all_chars = set(c for w in vocab for c in w)\n",
    "char2idx = {c: i+1 for i, c in enumerate(all_chars)} \n",
    "char2idx[\"<PAD>\"] = 0\n",
    "\n",
    "# Again this is an important step to tag the indexes to the labels or BIO tags\n",
    "tag2idx = {t: i for i, t in enumerate(label_list)}\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a60f59b",
   "metadata": {},
   "source": [
    "#### GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4681b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_glove(path, word2idx, dim=100):\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), dim))\n",
    "    embeddings[word2idx[\"<PAD>\"]] = np.zeros(dim)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            # vector representation for the words\n",
    "            if word in word2idx:\n",
    "                vector = np.array(parts[1:], dtype=np.float32)\n",
    "                embeddings[word2idx[word]] = vector\n",
    "    return torch.tensor(embeddings).float()\n",
    "\n",
    "# downloaded file - glove embedding 100 dimentional vector representation\n",
    "glove_path = r'C:\\Users\\pc\\Desktop\\Text Aalytics Twitter\\glove.6B.100d.txt'\n",
    "# embedding matrix\n",
    "embedding_matrix = load_glove(glove_path, word2idx, dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8260af6",
   "metadata": {},
   "source": [
    "#### Encoding Tokens, Tags, Characters, and POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9b9bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are coverting bio format strings to Numeric ID for training our model for pytorch and CRF \n",
    "def encode_tokens(tokens, word2idx, max_len):\n",
    "    return [word2idx.get(w.lower(), word2idx[\"<UNK>\"]) for w in tokens[:max_len]] +  [word2idx[\"<PAD>\"]] * (max_len - len(tokens))\n",
    "\n",
    "def encode_tags(tags, tag2idx, max_len):\n",
    "    return [tag2idx[t] for t in tags[:max_len]] + [tag2idx[\"O\"]] * (max_len - len(tags))\n",
    "\n",
    "def encode_chars(tokens, char2idx, max_len, max_char_len=15):\n",
    "    char_ids = []\n",
    "    # padding the tokens to max length\n",
    "    for word in tokens[:max_len]:\n",
    "        chars = [char2idx.get(c, 0) for c in word[:max_char_len]]\n",
    "        chars += [0] * (max_char_len - len(chars))\n",
    "        char_ids.append(chars)\n",
    "    for _ in range(max_len - len(char_ids)):\n",
    "        char_ids.append([0]*max_char_len)\n",
    "    return char_ids\n",
    "\n",
    "def encode_pos(tokens, max_len):\n",
    "    tags = [tag for _, tag in pos_tag(tokens[:max_len])]\n",
    "    tag2id = {tag: i+1 for i, tag in enumerate(set(tags))}\n",
    "    encoded = [tag2id[t] for t in tags]\n",
    "    return encoded + [0] * (max_len - len(encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d279e2",
   "metadata": {},
   "source": [
    "### NER Dataset - preprocessing and encoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "694631cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   named entity recognition datset to preprocess and encoding functions \n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, tokens, labels, word2idx, tag2idx, char2idx, max_len=50):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "        self.word2idx = word2idx\n",
    "        self.tag2idx = tag2idx\n",
    "        self.char2idx = char2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.tokens[idx]\n",
    "        tags = self.labels[idx]\n",
    "        x = encode_tokens(words, self.word2idx, self.max_len)\n",
    "        y = encode_tags(tags, self.tag2idx, self.max_len)\n",
    "        c = encode_chars(words, self.char2idx, self.max_len)\n",
    "        p = encode_pos(words, self.max_len)\n",
    "\n",
    "        return torch.tensor(x), torch.tensor(y), torch.tensor(c), torch.tensor(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb2907",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_set = NERDataset(train_tokens, train_labels, word2idx, tag2idx, char2idx, max_len)\n",
    "val_set = NERDataset(val_tokens, val_labels, word2idx, tag2idx, char2idx, max_len)\n",
    "test_set = NERDataset(test_tokens, test_labels, word2idx, tag2idx, char2idx, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6878a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "\n",
    "class BiLSTM_CRF_Model(nn.Module):\n",
    "\n",
    "    # word embedding, character embedding, CNN for char features, \n",
    "    # LSTM for sequence modelling, and CRF for decoding\n",
    "    def __init__(self, word_vocab_size, char_vocab_size, tagset_size, embedding_matrix, hidden_dim=128, char_dim=30, max_char_len=15):\n",
    "        super().__init__()\n",
    "        self.word_emb = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.char_emb = nn.Embedding(char_vocab_size, char_dim, padding_idx=0)\n",
    "        self.char_cnn = nn.Conv1d(char_dim, 30, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.size(1) + 30, hidden_dim // 2, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.crf = CRF(tagset_size)\n",
    "        self.max_char_len = max_char_len\n",
    "\n",
    "    # this function does forward Pass: Embeddings → LSTM \n",
    "\n",
    "    def forward(self, x_w, x_c, mask):\n",
    "        w_emb = self.word_emb(x_w)\n",
    "        b, l, c = x_c.shape\n",
    "        c_emb = self.char_emb(x_c.view(b*l, c)).permute(0, 2, 1)\n",
    "        c_feat = self.char_cnn(c_emb).max(dim=2)[0].view(b, l, -1)\n",
    "        feats = torch.cat([w_emb, c_feat], dim=2)\n",
    "        lstm_out, _ = self.lstm(feats)\n",
    "        emissions = self.fc(lstm_out)\n",
    "        return emissions\n",
    "    \n",
    "    # log-likelihood for CRF layer function \n",
    "    def loss(self, x_w, x_c, y, mask):\n",
    "        emissions = self.forward(x_w, x_c, mask)\n",
    "        return -self.crf(emissions, y, mask=mask).mean()  \n",
    "    \n",
    "    # CRF Viterbi Decoding\n",
    "\n",
    "    def predict(self, x_w, x_c, mask):\n",
    "        emissions = self.forward(x_w, x_c, mask)\n",
    "        \n",
    "        # masking boolean format for emmissions\n",
    "        if mask.dtype != torch.bool:\n",
    "            mask = mask.bool()\n",
    "        # returning the prediction of best tag sequence using CRF decoding\n",
    "        return self.crf.viterbi_decode(emissions, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99962b47",
   "metadata": {},
   "source": [
    "### Training for BiLSTM + CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f7b91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, epochs=5):\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y, c, p in tqdm(loader):  \n",
    "            mask = x != word2idx[\"<PAD>\"]\n",
    "            loss = model.loss(x, c, y, mask)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {e+1} Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f2621c",
   "metadata": {},
   "source": [
    "### Evaluation Function using Seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e731db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, accuracy_score\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y, c, p in loader:  \n",
    "            mask = x != word2idx[\"<PAD>\"]\n",
    "            preds = model.predict(x, c, mask)\n",
    "            for i in range(len(x)):\n",
    "                true = [idx2tag[t.item()] for t, m in zip(y[i], mask[i]) if m]\n",
    "                pred = [idx2tag[t] for t in preds[i]]\n",
    "                y_true.append(true)\n",
    "                y_pred.append(pred)\n",
    "\n",
    "    print(\"Classification Report BiLSTM + CRF:\")            \n",
    "    print(classification_report(y_true, y_pred))\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\nToken-level Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448e119",
   "metadata": {},
   "source": [
    "### Executing the BiLSTM + CRF model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb2c2be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [00:17<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1531.3741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [00:17<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 837.2660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [00:19<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 614.4080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [00:22<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 492.2237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [00:23<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 414.6241\n",
      "Classification Report BiLSTM + CRF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.68      0.36      0.47       636\n",
      "         ORG       0.50      0.23      0.32      1090\n",
      "         PER       0.75      0.87      0.81      2650\n",
      "\n",
      "   micro avg       0.71      0.64      0.67      4376\n",
      "   macro avg       0.64      0.49      0.53      4376\n",
      "weighted avg       0.68      0.64      0.64      4376\n",
      "\n",
      "\n",
      "Token-level Accuracy: 0.9371\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_CRF_Model(len(word2idx), len(char2idx), len(tag2idx), embedding_matrix)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train(model, train_loader, optimizer, epochs=5)\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a35d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Entity Span (BIO):\n",
      "Complete     -> O\n",
      "Tosca        -> O\n",
      "on           -> O\n",
      "the          -> O\n",
      "tube         -> O\n",
      "http://t.co/O90deSLB -> O\n"
     ]
    }
   ],
   "source": [
    "# entiy span in this model\n",
    "example_index = 3\n",
    "tokens = train_tokens[example_index]\n",
    "tags = train_labels[example_index]\n",
    "\n",
    "print(\"Example Entity Span (BIO):\")\n",
    "for token, tag in zip(tokens, tags):\n",
    "    print(f\"{token:<12} -> {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c125a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions and ground truths from BiLSTM+CRF\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y, c, p in test_loader:\n",
    "        mask = x != word2idx[\"<PAD>\"]\n",
    "        preds = model.predict(x, c, mask)\n",
    "        for i in range(len(x)):\n",
    "            true = [idx2tag[t.item()] for t, m in zip(y[i], mask[i]) if m]\n",
    "            pred = [idx2tag[t] for t in preds[i]]\n",
    "            y_true.append(true)\n",
    "            y_pred.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "146e8bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM + CRF Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.684524</td>\n",
       "      <td>0.361635</td>\n",
       "      <td>0.473251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.498054</td>\n",
       "      <td>0.234862</td>\n",
       "      <td>0.319202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.751460</td>\n",
       "      <td>0.873962</td>\n",
       "      <td>0.808095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     precision    recall  f1-score\n",
       "LOC   0.684524  0.361635  0.473251\n",
       "ORG   0.498054  0.234862  0.319202\n",
       "PER   0.751460  0.873962  0.808095"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "import pandas as pd\n",
    "\n",
    "# classification report for this model\n",
    "bilstm_report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "bilstm_results_df = pd.DataFrame(bilstm_report_dict).T\n",
    "bilstm_results_df = bilstm_results_df[['precision', 'recall', 'f1-score']]\n",
    "bilstm_results_df.drop(index=['micro avg', 'macro avg', 'weighted avg'], errors='ignore', inplace=True)\n",
    "\n",
    "print(\"BiLSTM + CRF Evaluation Table:\")\n",
    "display(bilstm_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad0c8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of BiLSTM+CRF Misclassification:\n",
      "\n",
      "Tokens       : ['This', 'morning', 'I', 'met', 'with', 'Senators', 'Inabo', 'and', 'Senior', 'from', 'Palau', 'to', 'discuss', 'my', 'role', 'as', 'Chair', 'of', 'the', 'Public', 'Works', '.', '.', '.', 'http://t.co/McYNwpzHmt']\n",
      "True Labels  : ['O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Pred Labels  : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of BiLSTM+CRF Misclassification:\")\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i] != y_pred[i]:\n",
    "        print(f\"\\nTokens       : {test_tokens[i]}\")\n",
    "        print(f\"True Labels  : {y_true[i]}\")\n",
    "        print(f\"Pred Labels  : {y_pred[i]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f15e9",
   "metadata": {},
   "source": [
    "## TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d69789bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\pc\\anaconda3\\envs\\text_analytics\\lib\\site-packages\\huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets seqeval torch accelerate\n",
    "# installing the nexesary packages for the TinyBERT model for the NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e223963d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abad8a2e18ff4af7a3d38679025beb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "from seqeval.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# once again loading the dataset for the TinyBERT model\n",
    "btc = load_dataset(\"tner/btc\")\n",
    "label_list = btc[\"train\"].features[\"tags\"].feature.names\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# tokenizer for Tiny BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = []\n",
    "        prev_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned_labels.append(-100)\n",
    "            elif word_id != prev_word_id:\n",
    "                aligned_labels.append(label[word_id])\n",
    "            else:\n",
    "                aligned_labels.append(-100)\n",
    "            prev_word_id = word_id\n",
    "        labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# the tokenized data\n",
    "tokenized_datasets = btc.map(tokenize_and_align_labels, batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Loading the \"prajjwal1/bert-tiny\" tinybert for modelling\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\", \n",
    "    num_labels=num_labels, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c8effa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "# here we are defining the computing the metrixs for the classification report for the TinyBERT model\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    print(\"Classification Report for TinyBERT:\")\n",
    "    print(classification_report(true_labels, true_predictions))\n",
    "    return {\n",
    "        \"precision\": classification_report(true_labels, true_predictions, output_dict=True)[\"weighted avg\"][\"precision\"],\n",
    "        \"recall\": classification_report(true_labels, true_predictions, output_dict=True)[\"weighted avg\"][\"recall\"],\n",
    "        \"f1\": classification_report(true_labels, true_predictions, output_dict=True)[\"weighted avg\"][\"f1-score\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "093b34bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_35952\\3837342338.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3965' max='3965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3965/3965 03:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.309184</td>\n",
       "      <td>0.511844</td>\n",
       "      <td>0.515607</td>\n",
       "      <td>0.501930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>0.273120</td>\n",
       "      <td>0.623220</td>\n",
       "      <td>0.531792</td>\n",
       "      <td>0.530990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.248536</td>\n",
       "      <td>0.665381</td>\n",
       "      <td>0.577457</td>\n",
       "      <td>0.560566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.288100</td>\n",
       "      <td>0.246562</td>\n",
       "      <td>0.637005</td>\n",
       "      <td>0.565318</td>\n",
       "      <td>0.567030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.202600</td>\n",
       "      <td>0.243664</td>\n",
       "      <td>0.646460</td>\n",
       "      <td>0.571676</td>\n",
       "      <td>0.573592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for TinyBERT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\envs\\text_analytics\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.47      0.10      0.16       156\n",
      "         ORG       0.00      0.00      0.00       380\n",
      "         PER       0.68      0.73      0.71      1194\n",
      "\n",
      "   micro avg       0.68      0.52      0.58      1730\n",
      "   macro avg       0.38      0.28      0.29      1730\n",
      "weighted avg       0.51      0.52      0.50      1730\n",
      "\n",
      "Classification Report for TinyBERT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.48      0.23      0.31       156\n",
      "         ORG       0.44      0.02      0.04       380\n",
      "         PER       0.70      0.73      0.72      1194\n",
      "\n",
      "   micro avg       0.69      0.53      0.60      1730\n",
      "   macro avg       0.54      0.33      0.35      1730\n",
      "weighted avg       0.62      0.53      0.53      1730\n",
      "\n",
      "Classification Report for TinyBERT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.47      0.27      0.34       156\n",
      "         ORG       0.62      0.04      0.07       380\n",
      "         PER       0.70      0.79      0.74      1194\n",
      "\n",
      "   micro avg       0.69      0.58      0.63      1730\n",
      "   macro avg       0.60      0.37      0.39      1730\n",
      "weighted avg       0.67      0.58      0.56      1730\n",
      "\n",
      "Classification Report for TinyBERT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.46      0.27      0.34       156\n",
      "         ORG       0.37      0.03      0.05       380\n",
      "         PER       0.75      0.77      0.76      1194\n",
      "\n",
      "   micro avg       0.72      0.57      0.63      1730\n",
      "   macro avg       0.52      0.36      0.38      1730\n",
      "weighted avg       0.64      0.57      0.57      1730\n",
      "\n",
      "Classification Report for TinyBERT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.46      0.27      0.34       156\n",
      "         ORG       0.41      0.04      0.07       380\n",
      "         PER       0.75      0.78      0.76      1194\n",
      "\n",
      "   micro avg       0.72      0.57      0.64      1730\n",
      "   macro avg       0.54      0.36      0.39      1730\n",
      "weighted avg       0.65      0.57      0.57      1730\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3965, training_loss=0.3112707640153613, metrics={'train_runtime': 232.8124, 'train_samples_per_second': 136.118, 'train_steps_per_second': 17.031, 'total_flos': 3565420121496.0, 'train_loss': 0.3112707640153613, 'epoch': 5.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the TinyBERT Model using 5 epochs \n",
    "args = TrainingArguments(\n",
    "    output_dir=\"tinybert-btc-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset = tokenized_datasets[\"train\"], \n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27870722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report TinyBERT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC      0.755     0.363     0.490       636\n",
      "         ORG      0.633     0.236     0.344      1090\n",
      "         PER      0.776     0.876     0.823      2650\n",
      "\n",
      "   micro avg      0.759     0.642     0.696      4376\n",
      "   macro avg      0.721     0.492     0.552      4376\n",
      "weighted avg      0.738     0.642     0.655      4376\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# here we are defining the computing the metrixs for the classification report for the TinyBERT model\n",
    "# Filtering out 'O' taht is the other entities labels from both y_true and y_pred\n",
    "def filter_out_O(y_true, y_pred):\n",
    "    new_y_true, new_y_pred = [], []\n",
    "    for t_seq, p_seq in zip(y_true, y_pred):\n",
    "        filtered_t, filtered_p = [], []\n",
    "        for t, p in zip(t_seq, p_seq):\n",
    "            if t != 'O':\n",
    "                filtered_t.append(t)\n",
    "                filtered_p.append(p)\n",
    "        if filtered_t:  # ensure we don't add empty rows\n",
    "            new_y_true.append(filtered_t)\n",
    "            new_y_pred.append(filtered_p)\n",
    "    return new_y_true, new_y_pred\n",
    "filtered_true, filtered_pred = filter_out_O(y_true, y_pred)\n",
    "\n",
    "# Run evaluation without 'O'\n",
    "print(\"Classification Report TinyBERT:\")\n",
    "print(classification_report(filtered_true, filtered_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b33cee49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete     -> O\n",
      "Tosca        -> O\n",
      "on           -> O\n",
      "the          -> O\n",
      "tube         -> O\n",
      "http://t.co/O90deSLB -> O\n"
     ]
    }
   ],
   "source": [
    "# example of the entity spans for this model\n",
    "example_index = 3\n",
    "tokens = btc['train']['tokens'][example_index]\n",
    "labels = btc['train']['tags'][example_index]\n",
    "bio_tags = [label_list[t] for t in labels]\n",
    "for token, tag in zip(tokens, bio_tags):\n",
    "    print(f\"{token:<12} -> {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "978b8dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Tiny BERT Misclassification:\n",
      "\n",
      "Tokens       : ['This', 'morning', 'I', 'met', 'with', 'Senators', 'Inabo', 'and', 'Senior', 'from', 'Palau', 'to', 'discuss', 'my', 'role', 'as', 'Chair', 'of', 'the', 'Public', 'Works', '.', '.', '.', 'http://t.co/McYNwpzHmt']\n",
      "True Labels  : ['B-PER', 'B-PER', 'B-LOC']\n",
      "Pred Labels  : ['O', 'O', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of Tiny BERT Misclassification:\")\n",
    "for i in range(len(filtered_true)):\n",
    "    if filtered_true[i] != filtered_pred[i]:\n",
    "        print(f\"\\nTokens       : {btc['test']['tokens'][i]}\")\n",
    "        print(f\"True Labels  : {filtered_true[i]}\")\n",
    "        print(f\"Pred Labels  : {filtered_pred[i]}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83709dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# F1 scores for each model per entity\n",
    "data = {\n",
    "    \"Entity\": [\"LOC\", \"ORG\", \"PER\"],\n",
    "    \"CRF\": [0.527, 0.375, 0.812],\n",
    "    \"BiLSTM+CRF\": [0.44, 0.35, 0.82],\n",
    "    \"Tiny BERT\": [0.461, 0.383, 0.827]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "bar_width = 0.25\n",
    "x = range(len(df[\"Entity\"]))\n",
    "\n",
    "# plotiing a bar chart for all the models with the F1 Scores as the performance metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([p - bar_width for p in x], df[\"CRF\"], width=bar_width, label=\"CRF\", color='#C8A2C8') \n",
    "plt.bar(x, df[\"BiLSTM+CRF\"], width=bar_width, label=\"BiLSTM+CRF\", color='#FFC0CB') \n",
    "plt.bar([p + bar_width for p in x], df[\"Tiny BERT\"], width=bar_width, label=\"Tiny BERT\", color='#87CEEB')\n",
    "\n",
    "# Add labels and formatting\n",
    "plt.xlabel(\"Entity Type\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Model Comparison on F1-score (Excluding 'O')\")\n",
    "plt.xticks(ticks=x, labels=df[\"Entity\"])\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
